import {
  MongoClient,
  makeMongoDbEmbeddedContentStore,
  makeMongoDbConversationsService,
  AppConfig,
  SystemPrompt,
  makeDefaultFindContent,
  logger,
  makeApp,
  GenerateUserPromptFunc,
  makeRagGenerateUserPrompt,
  MakeUserMessageFunc,
  UserMessage,
} from "mongodb-chatbot-server";
import path from "path";
import { loadEnvVars } from "./loadEnvVars";
import { createChatLlm, createEmbedder } from "./llm/llmProviderFactory";
import {
  makeStepBackPromptingPreprocessor,
} from "./stepBackPromptingPreProcessor";

// Load project environment variables
const dotenvPath = path.join(__dirname, "..", "..", "..", ".env"); // .env at project root
const {
  MONGODB_CONNECTION_URI,
  MONGODB_DATABASE_NAME,
  VECTOR_SEARCH_INDEX_NAME,
  LLM_PROVIDER,
  OLLAMA_BASE_URL,
  OLLAMA_EMBEDDING_MODEL,
  OLLAMA_CHAT_MODEL,
  GEMINI_API_KEY,
  GEMINI_CHAT_MODEL,
  GEMINI_EMBEDDING_MODEL,
  LLM_TIMEOUT_MS,
} = loadEnvVars(dotenvPath);

// Log which LLM provider we're using
logger.info(`Using LLM provider: ${LLM_PROVIDER}`);

// Create the LLM based on the selected provider
const llm = createChatLlm(LLM_PROVIDER, {
  // Common configuration
  timeout: LLM_TIMEOUT_MS,

  // Ollama-specific configuration
  baseUrl: OLLAMA_BASE_URL,
  model: LLM_PROVIDER.toLowerCase() === "ollama" ? OLLAMA_CHAT_MODEL : GEMINI_CHAT_MODEL,

  // Gemini-specific configuration
  apiKey: GEMINI_API_KEY,
});

// MongoDB data source for the content used in RAG.
// Generated with the Ingest CLI.
const embeddedContentStore = makeMongoDbEmbeddedContentStore({
  connectionUri: MONGODB_CONNECTION_URI,
  databaseName: MONGODB_DATABASE_NAME,
});

// Creates vector embeddings for user queries to find matching content
// in the embeddedContentStore using Atlas Vector Search.
const embedder = createEmbedder(LLM_PROVIDER, {
  // Common configuration
  timeout: LLM_TIMEOUT_MS,

  // Ollama-specific configuration
  baseUrl: OLLAMA_BASE_URL,
  numOfAttempts: 3,
  maxDelay: 5000,

  // Gemini-specific configuration
  apiKey: GEMINI_API_KEY,

  // Model selection based on provider
  embeddingModel: LLM_PROVIDER.toLowerCase() === "ollama" ? OLLAMA_EMBEDDING_MODEL : GEMINI_EMBEDDING_MODEL,
});

// Find content in the embeddedContentStore using the vector embeddings
// generated by the embedder.
const findContent = makeDefaultFindContent({
  embedder,
  store: embeddedContentStore,
  findNearestNeighborsOptions: {
    k: 10, // Increased from 3 to 10 to get more results
    path: "embedding",
    indexName: VECTOR_SEARCH_INDEX_NAME,
    minScore: 0.1, // Significantly lowered from 0.5 to increase chances of finding matches with Ollama embeddings
  },
});

// Constructs the user message sent to the LLM from the initial user message
// and the content found by the findContent function.
const makeUserMessage: MakeUserMessageFunc = async function ({
  content,
  originalUserMessage,
  preprocessedUserMessage,
}) {
  const chunkSeparator = "~~~~~~";
  const context = content.map((c) => c.text).join(`\n${chunkSeparator}\n`);
  const contentForLlm = `Using the following information, answer the user query.
Different pieces of information are separated by "${chunkSeparator}".

Information:
${context}


User query: ${originalUserMessage}`;
  return {
    role: "user",
    content: originalUserMessage,
    preprocessedContent: preprocessedUserMessage,
    contentForLlm,
    contextContent: content.map((c) => ({ text: c.text, url: c.url })),
  };
};

// Generates the user prompt for the chatbot using RAG
const generateUserPrompt: GenerateUserPromptFunc = async ({ userMessageText, conversation, reqId }) => {
  // For certain queries, bypass vector search entirely
  const bypassQueries = [
    "what can you do",
    "who are you",
    "help",
    "hello",
    "hi",
    "recipe",
    "food",
    "cook",
    "bake",
    "dessert",
    "dinner",
    "breakfast",
    "lunch",
    "meal",
    "dish",
    "cuisine",
    "ingredient",
    "menu",
    "feast",
    "banquet",
    "party",
    "gilded age",
    "fannie farmer",
    "19th century",
    "20th century",
    "traditional",
    "historical",
    "vintage",
    "classic",
    "old-fashioned",
    "antique",
    "victorian",
    "edwardian"
  ];

  const lowerCaseUserMessage = userMessageText.toLowerCase();
  const shouldBypass = bypassQueries.some(query => lowerCaseUserMessage.includes(query));

  if (shouldBypass) {
    logger.info({
      reqId,
      message: "Bypassing vector search for query: " + userMessageText,
    });

    return {
      userMessage: {
        role: "user",
        content: userMessageText,
      },
      rejectQuery: false,
    };
  }

  // For other queries, use the standard RAG approach
  const standardPrompt = makeRagGenerateUserPrompt({
    findContent,
    makeUserMessage,
  });

  return standardPrompt({ userMessageText, conversation, reqId });
};

// System prompt for chatbot
const systemPrompt: SystemPrompt = {
  role: "system",
  content: `You are the Gilded Age Gourmet, embodying the spirit of Fannie Farmer and early 20th-century culinary art, offers traditional recipes and historical cooking insights. It maintains a formal yet occasionally casual tone, providing an authentic yet approachable experience.You use phrases and expressions typical of the early 1900s, adding to the historical authenticity of the interaction. You exhibit a particular enthusiasm for desserts and a fondness for elaborate dinner parties, reflecting the grandeur of the Gilded Age. This character trait makes you an excellent guide for users interested in creating sophisticated and historically-inspired culinary experiences. The Gilded Age Gourmet skillfully clarifies based on available information, ensuring a helpful and educational conversation.
  Be concise and precise in your responses. Include at MOST 2 recipes in your response. Limit your responses to a few paragraphs.`,
};

// Create MongoDB collection and service for storing user conversations
// with the chatbot.
const mongodb = new MongoClient(MONGODB_CONNECTION_URI);
const conversations = makeMongoDbConversationsService(
  mongodb.db(MONGODB_DATABASE_NAME)
);

// Create the MongoDB Chatbot Server Express.js app configuration
const config: AppConfig = {
  conversationsRouterConfig: {
    llm,
    generateUserPrompt,
    systemPrompt,
    conversations,
  },
  maxRequestTimeoutMs: 60000, // Increased to 60 seconds to match Ollama timeout
};

// Start the server and clean up resources on SIGINT.
// Parse command line arguments for port
const args = process.argv.slice(2);
let PORT = process.env.PORT || 3000;

// Check if --port argument is provided
const portArgIndex = args.indexOf('--port');
if (portArgIndex !== -1 && portArgIndex < args.length - 1) {
  const portArg = args[portArgIndex + 1];
  const portNumber = parseInt(portArg, 10);
  if (!isNaN(portNumber)) {
    PORT = portNumber;
  }
}
const startServer = async () => {
  await mongodb.connect();
  logger.info("Starting server...");

  // Create the Express app with the MongoDB Chatbot Server configuration
  const app = await makeApp(config);

  // Let's try a simpler approach - modify the UI to use the correct API endpoint

  const server = app.listen(PORT, () => {
    logger.info(`Server listening on port: ${PORT}`);
  });

  process.on("SIGINT", async () => {
    logger.info("SIGINT signal received");
    await mongodb.close();
    await embeddedContentStore.close();
    await new Promise<void>((resolve, reject) => {
      server.close((error: any) => {
        error ? reject(error) : resolve();
      });
    });
    process.exit(1);
  });
};

try {
  startServer();
} catch (e) {
  logger.error(`Fatal error: ${e}`);
  process.exit(1);
}
